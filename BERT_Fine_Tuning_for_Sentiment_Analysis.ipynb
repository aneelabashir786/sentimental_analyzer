{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT Fine-Tuning for Sentiment Analysis\n",
        "\n",
        "This code fine-tunes a BERT (Bidirectional Encoder Representations from Transformers) model for binary sentiment classification (Positive / Negative) using your custom dataset sentiment-analysis.csv.\n",
        "\n",
        "It follows a complete data-to-model pipeline in ten structured steps:\n",
        "\n",
        " 1. Environment Setup\n",
        "\n",
        "Installs all required libraries (torch, transformers, scikit-learn, etc.).\n",
        "\n",
        "Disables W&B logging for clean output.\n",
        "\n",
        "Sets reproducible random seeds for consistent results.\n",
        "\n",
        " 2. Data Loading & Cleaning\n",
        "\n",
        "Reads the CSV dataset (sentiment-analysis.csv) and handles stray commas or quotes.\n",
        "\n",
        "Cleans column names and text (removes spaces, converts to lowercase).\n",
        "\n",
        "Ensures your dataset has two main columns:\n",
        "Text → the review/sentence\n",
        "Sentiment → the label (positive or negative).\n",
        "\n",
        " 3. Label Mapping\n",
        "\n",
        "Converts sentiment strings into numeric labels using:\n",
        "\n",
        "{'negative': 0, 'positive': 1}\n",
        "\n",
        "\n",
        "Drops rows with missing or unmapped labels.\n",
        "\n",
        "Checks class balance and removes duplicates.\n",
        "\n",
        " 4. Train-Test Split\n",
        "\n",
        "Splits data into training (80%) and testing (20%) sets stratified by label, ensuring both classes are balanced in both splits.\n",
        "\n",
        "Verifies that there’s no data leakage (no same text appears in both sets).\n",
        "\n",
        " 5. Tokenization\n",
        "\n",
        "Uses the BERT tokenizer (bert-base-uncased) to:\n",
        "\n",
        "Split sentences into tokens.\n",
        "\n",
        "Add special [CLS] and [SEP] tokens.\n",
        "\n",
        "Pad/truncate sequences to max_length=128.\n",
        "\n",
        "Produces token IDs and attention masks for model input.\n",
        "\n",
        " 6. Dataset Preparation\n",
        "\n",
        "Defines a custom SentimentDataset class that:\n",
        "\n",
        "Wraps encoded inputs and labels into PyTorch tensors.\n",
        "\n",
        "Allows the Hugging Face Trainer API to use them directly.\n",
        "\n",
        " 7. Model Initialization\n",
        "\n",
        "Loads BertForSequenceClassification from Hugging Face with:\n",
        "\n",
        "A pre-trained BERT encoder (bert-base-uncased).\n",
        "\n",
        "A new classification head (2 output neurons for positive/negative).\n",
        "\n",
        " 8. Training Configuration\n",
        "\n",
        "Sets up TrainingArguments with:\n",
        "\n",
        "2 epochs.\n",
        "\n",
        "Batch size = 8.\n",
        "\n",
        "Evaluation after every epoch.\n",
        "\n",
        "Logging and output directories.\n",
        "\n",
        "Fixed random seed.\n",
        "\n",
        " 9. Model Training & Evaluation\n",
        "\n",
        "Fine-tunes BERT on your labeled dataset using the Hugging Face Trainer.\n",
        "\n",
        "Evaluates model performance on the test set using:\n",
        "\n",
        "Accuracy\n",
        "\n",
        "F1-score\n",
        "\n",
        "Confusion Matrix\n",
        "\n",
        "Classification Report\n",
        "\n",
        "Also lists a few misclassified examples for quick inspection.\n",
        "\n",
        " 10. Inference / Prediction\n",
        "\n",
        "Demonstrates real-time inference with sample texts:\n",
        "\n",
        "\"I absolutely loved this product!\" → Positive\n",
        "\"This was a waste of money.\" → Negative\n",
        "\n",
        "\n",
        "Tokenizes and passes them to the trained model for prediction.\n",
        "\n",
        " End Result\n",
        "\n",
        "By the end, you get:\n",
        "\n",
        "A fine-tuned BERT sentiment classifier.\n",
        "\n",
        "Model evaluation metrics.\n",
        "\n",
        "Ready-to-use inference logic for new unseen text."
      ],
      "metadata": {
        "id": "rvrtGgQF2rXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch torchvision torchaudio transformers scikit-learn pandas numpy\n"
      ],
      "metadata": {
        "id": "hjpoZk127aFc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Libraries"
      ],
      "metadata": {
        "id": "pOyjUwfU7Zmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "3FHCS2-d7dXM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reproducibility seeds\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)"
      ],
      "metadata": {
        "id": "LDikTa6T89-R"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Dataset\n"
      ],
      "metadata": {
        "id": "Vq5zSQWC9BXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"sentiment-analysis.csv\", sep=r'\\s*,\\s*', engine='python')"
      ],
      "metadata": {
        "id": "yyU33xq-9E7X"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "QWGh31gb9WTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = [c.strip().replace('\"', '') for c in df.columns]\n",
        "\n",
        "\n",
        "# This converts all string values in the DF to lowercase and strips spaces.\n",
        "df = df.applymap(lambda x: x.lower().strip() if isinstance(x, str) else x)\n",
        "\n",
        "print(df.columns)\n",
        "# Remove stray quotes in the Text column if any\n",
        "if 'Text' in df.columns:\n",
        "    df['Text'] = df['Text'].astype(str).str.replace('\"', '', regex=False).str.strip()\n",
        "\n",
        "# Standardize Sentiment column name and values\n",
        "if 'Sentiment' not in df.columns and 'Sentiment' in df.columns:\n",
        "    df.rename(columns={'Sentiment':'Sentiment'}, inplace=True)\n",
        "\n",
        "# Print sample and counts\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "if 'Text' in df.columns and 'Sentiment' in df.columns:\n",
        "    print(\"Example rows:\\n\", df[['Text','Sentiment']].head())\n",
        "    print(\"Label counts:\\n\", df['Sentiment'].value_counts())\n",
        "\n",
        "# -------------------------\n",
        "# 2) Map labels robustly (support lowercase/uppercase)\n",
        "# Accept common variants\n",
        "label_map = {'negative': 0, 'positive': 1, 'neg':0, 'pos':1}\n",
        "# make sure sentiments are strings and stripped\n",
        "df['Sentiment'] = df['Sentiment'].astype(str).str.strip().str.lower()\n",
        "df['label'] = df['Sentiment'].map(label_map)\n",
        "\n",
        "# Drop rows that failed mapping and any missing text\n",
        "before = len(df)\n",
        "df = df.dropna(subset=['Text','label'])\n",
        "df['label'] = df['label'].astype(int)\n",
        "after = len(df)\n",
        "print(f\"Dropped {before-after} rows due to unmapped labels or missing text.\")\n",
        "\n",
        "# quick balance check\n",
        "print(\"Label distribution after mapping:\\n\", df['label'].value_counts())\n",
        "\n",
        "# -------------------------\n",
        "# 3) Sanity checks to detect leakage or duplicates\n",
        "# check duplicates globally\n",
        "dups = df.duplicated(subset=['Text','label']).sum()\n",
        "print(\"Total duplicated (text,label) rows in DF:\", dups)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvD6x6Uh9MH0",
        "outputId": "d5a229d7-0d05-45b3-a938-2ad1195ab6d7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Text', 'Sentiment', 'Source', 'Date/Time', 'User ID', 'Location',\n",
            "       'Confidence Score'],\n",
            "      dtype='object')\n",
            "Columns: ['Text', 'Sentiment', 'Source', 'Date/Time', 'User ID', 'Location', 'Confidence Score']\n",
            "Example rows:\n",
            "                                                Text Sentiment\n",
            "0                              i love this product!  positive\n",
            "1                         the service was terrible.  negative\n",
            "2                            this movie is amazing!  positive\n",
            "3  i'm so disappointed with their customer support.  negative\n",
            "4                just had the best meal of my life!  positive\n",
            "Label counts:\n",
            " Sentiment\n",
            "positive    53\n",
            "negative    43\n",
            "Name: count, dtype: int64\n",
            "Dropped 2 rows due to unmapped labels or missing text.\n",
            "Label distribution after mapping:\n",
            " label\n",
            "1    53\n",
            "0    43\n",
            "Name: count, dtype: int64\n",
            "Total duplicated (text,label) rows in DF: 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-292163358.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: x.lower().strip() if isinstance(x, str) else x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train/Test Splitting\n"
      ],
      "metadata": {
        "id": "3entlls19oYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll do a train/test split with stratify (if >1 class)\n",
        "if df['label'].nunique() < 2:\n",
        "    raise ValueError(\"Need at least 2 label classes to train. Found: \" + str(df['label'].unique()))\n",
        "\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['Text'].tolist(),\n",
        "    df['label'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=SEED,\n",
        "    stratify=df['label']\n",
        ")\n",
        "\n",
        "# Ensure no exact overlap between train and test\n",
        "train_set = set(train_texts)\n",
        "test_set = set(test_texts)\n",
        "overlap = train_set.intersection(test_set)\n",
        "print(\"Train/Test exact overlap count (should be 0):\", len(overlap))\n",
        "if len(overlap) > 0:\n",
        "    print(\"Example overlapping texts:\", list(overlap)[:3])\n",
        "\n",
        "# Show label distribution in splits\n",
        "import collections\n",
        "print(\"Train label counts:\", collections.Counter(train_labels))\n",
        "print(\"Test label counts:\", collections.Counter(test_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9KK8Da89sB8",
        "outputId": "466ce403-323e-4d57-9ebb-3bc277326113"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Test exact overlap count (should be 0): 6\n",
            "Example overlapping texts: [\"i can't stop listening to this song. it's my new favorite!\", 'the website loading speed is frustratingly slow. needs improvement.', \"just had the most amazing vacation! i can't wait to go back.\"]\n",
            "Train label counts: Counter({1: 42, 0: 34})\n",
            "Test label counts: Counter({1: 11, 0: 9})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ],
      "metadata": {
        "id": "rmhr0vVJ9tR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Tokenizer (return lists, not tensors) & encodings\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# produce dicts of lists (no return_tensors)\n",
        "train_enc = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
        "test_enc  = tokenizer(test_texts, truncation=True, padding=True, max_length=128)\n",
        "\n",
        "\n",
        "# Dataset wrapper that converts to tensors once (avoid double-conversion)\n",
        "class SentimentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx], dtype=torch.long) for k, v in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = SentimentDataset(train_enc, train_labels)\n",
        "test_dataset  = SentimentDataset(test_enc, test_labels)"
      ],
      "metadata": {
        "id": "96RW6iKO9vG5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "SqplMUWt_fss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NaNBS7w99xn",
        "outputId": "0623d4ea-b017-4ee5-856e-46cccb64ee21"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "p_SHg1Np_ik3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=50,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_strategy='epoch',\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='no',\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# 8) Train\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "model.save_pretrained(\"sentiment_model\")\n",
        "tokenizer.save_pretrained(\"sentiment_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "WOexy1VS_nY4",
        "outputId": "b1cd5043-42b3-40a1-edd9-6e3785eac07c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 01:20, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.683700</td>\n",
              "      <td>0.644455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.585500</td>\n",
              "      <td>0.458603</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('sentiment_model/tokenizer_config.json',\n",
              " 'sentiment_model/special_tokens_map.json',\n",
              " 'sentiment_model/vocab.txt',\n",
              " 'sentiment_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "zD9OqW6A_lo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = trainer.predict(test_dataset)\n",
        "preds = np.argmax(predictions.predictions, axis=1)\n",
        "\n",
        "\n",
        "print(\"\\n Evaluation Results\")\n",
        "print(\"Accuracy:\", accuracy_score(test_labels, preds))\n",
        "print(\"F1-Score (macro):\", f1_score(test_labels, preds, average='macro'))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(test_labels, preds))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(test_labels, preds, target_names=['Negative', 'Positive']))\n",
        "\n",
        "# Quick mismatch examples (if any)\n",
        "mismatch_idx = [i for i,(t,p) in enumerate(zip(test_texts, preds)) if p != test_labels[i]]\n",
        "print(\"Number of mismatches on test set:\", len(mismatch_idx))\n",
        "if len(mismatch_idx) > 0:\n",
        "    for idx in mismatch_idx[:5]:\n",
        "        print(\"TEXT:\", test_texts[idx], \"TRUE:\", test_labels[idx], \"PRED:\", preds[idx])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "R2ZkGRfH_r4P",
        "outputId": "1a1feb77-11f8-4d6d-f75e-d56543a6312b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Evaluation Results\n",
            "Accuracy: 0.9\n",
            "F1-Score (macro): 0.8958333333333333\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 7  2]\n",
            " [ 0 11]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    Negative       1.00      0.78      0.88         9\n",
            "    Positive       0.85      1.00      0.92        11\n",
            "\n",
            "    accuracy                           0.90        20\n",
            "   macro avg       0.92      0.89      0.90        20\n",
            "weighted avg       0.92      0.90      0.90        20\n",
            "\n",
            "Number of mismatches on test set: 2\n",
            "TEXT: the product i received was damaged. unacceptable. TRUE: 0 PRED: 1\n",
            "TEXT: the website loading speed is frustratingly slow. needs improvement. TRUE: 0 PRED: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example"
      ],
      "metadata": {
        "id": "Ep7Ou6PM_zIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "model_path = \"sentiment_model\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "texts = [\n",
        "    \"I love this phone, it's amazing!\",\n",
        "    \"I hate this, worst experience ever.\"\n",
        "]\n",
        "\n",
        "for text in texts:\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "        pred = torch.argmax(probs, dim=1).item()\n",
        "        print(text, \"→\", \"Positive\" if pred == 1 else \"Negative\", f\"({probs[0][pred]:.2f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExFLKOiJfbPr",
        "outputId": "ec1fb160-3c0e-414c-9b50-24f01b87f8a3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I love this phone, it's amazing! → Positive (0.74)\n",
            "I hate this, worst experience ever. → Negative (0.52)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L0KfY6FP6HJw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}